

class Transformer:
    def __init__(self):
        pass

    def scaled_dot_product_attention(self,):
        pass 

    def multi_head_attention(self,):
        pass 

    def positionwise_feedforward(self,):
        pass

    def positional_encoding(self,):
        pass